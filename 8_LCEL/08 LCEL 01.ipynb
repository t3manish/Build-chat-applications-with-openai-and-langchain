{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59c4b4ce",
      "metadata": {
        "id": "59c4b4ce"
      },
      "source": [
        "# Piping a Prompt, Model, and an Output Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c55c34-f55d-4ec7-8d47-ba36e6789c90",
      "metadata": {
        "id": "d9c55c34-f55d-4ec7-8d47-ba36e6789c90"
      },
      "outputs": [],
      "source": [
        "# Run the line of code below to check the version of langchain in the current environment.\n",
        "# Substitute \"langchain\" with any other package name to check their version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4cd3a5f1-493e-4c27-80b0-de820a039f59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cd3a5f1-493e-4c27-80b0-de820a039f59",
        "outputId": "e9288311-1b41-4b8f-cd3e-9933525fd591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6df5bb",
      "metadata": {
        "id": "7a6df5bb"
      },
      "outputs": [],
      "source": [
        "# %load_ext dotenv\n",
        "# %dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "55LuO67dc53E"
      },
      "id": "55LuO67dc53E",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "4oWFhJ93c8kP"
      },
      "id": "4oWFhJ93c8kP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d817fd5c",
      "metadata": {
        "id": "d817fd5c"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cb4ba189",
      "metadata": {
        "id": "cb4ba189"
      },
      "outputs": [],
      "source": [
        "list_instructions = CommaSeparatedListOutputParser().get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8513454e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8513454e",
        "outputId": "18cd3e7f-5c42-4150-bca8-1876cab7d74d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "list_instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So below ChatPromptTemplate is automatically defines as human prompt"
      ],
      "metadata": {
        "id": "JpmN0nBdgvuR"
      },
      "id": "JpmN0nBdgvuR"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f275facd",
      "metadata": {
        "id": "f275facd"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    ('human',\n",
        "     \"I've recently adopted a {pet}. Could you suggest three {pet} names? \\n\" + list_instructions)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "70c4e56f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70c4e56f",
        "outputId": "4253a132-3825-4dfd-c421-ef39ee12aed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I've recently adopted a {pet}. Could you suggest three {pet} names? \n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
          ]
        }
      ],
      "source": [
        "print(chat_template.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "694acaa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694acaa9",
        "outputId": "e03f28e6-96be-44e9-ed87-64533a15c22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ],
      "source": [
        "chat = ChatOpenAI(model_name = 'gpt-4',\n",
        "                  model_kwargs = {'seed':365},\n",
        "                  temperature = 0,\n",
        "                  max_tokens = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "20d2f81b",
      "metadata": {
        "id": "20d2f81b"
      },
      "outputs": [],
      "source": [
        "list_output_parser = CommaSeparatedListOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So passing the value \"dog\" manually and checking the prompt"
      ],
      "metadata": {
        "id": "Y1nCKwkXhyKI"
      },
      "id": "Y1nCKwkXhyKI"
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template.invoke({'pet':'dog'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLp0Tc6Lg-Sx",
        "outputId": "7ca8fc68-8bcb-4f24-a191-a4e3ad785182"
      },
      "id": "kLp0Tc6Lg-Sx",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"I've recently adopted a dog. Could you suggest three dog names? \\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "80490589",
      "metadata": {
        "id": "80490589"
      },
      "outputs": [],
      "source": [
        "chat_template_result = chat_template.invoke({'pet':'dog'})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see we have passed to chatgpt then got three names"
      ],
      "metadata": {
        "id": "KQ_Ifad9hgRG"
      },
      "id": "KQ_Ifad9hgRG"
    },
    {
      "cell_type": "code",
      "source": [
        "chat.invoke(chat_template_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhZxYhdUhGXI",
        "outputId": "23154d25-1b9e-4126-b898-ba9787b88084"
      },
      "id": "PhZxYhdUhGXI",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Max, Bella, Cooper', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 50, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-Caxbq0WhmYfAiB3pLjvTDrIpJAHl5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--cf1443fe-d023-4f37-8856-07fda9a5dc8e-0', usage_metadata={'input_tokens': 50, 'output_tokens': 5, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "67817365",
      "metadata": {
        "id": "67817365"
      },
      "outputs": [],
      "source": [
        "chat_result = chat.invoke(chat_template_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3ddff78f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ddff78f",
        "outputId": "6721aed5-03e4-4655-ec8e-2600a81ab410"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Max', 'Bella', 'Charlie']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "list_output_parser.invoke(chat_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chain = chat_template | chat | list_output_parser creates a LangChain Expression Language (LCEL) chain by combining a prompt template, a language model, and an output parser. Here's what each part does:\n",
        "\n",
        "chat_template (ChatPromptTemplate): This is the prompt that defines the structure and content of the input to the language model. It takes a variable (in this case, {pet}) and inserts it into a predefined message, also including instructions for the output format from list_instructions.\n",
        "\n",
        "chat (ChatOpenAI): This is the large language model (LLM) itself. It takes the structured prompt generated by chat_template as input and produces a response based on its training.\n",
        "\n",
        "list_output_parser (CommaSeparatedListOutputParser): This component takes the raw output from the chat model and processes it. In this specific case, it expects a comma-separated string from the model and converts it into a Python list of strings.\n",
        "\n",
        "The | operator pipes the output of one component as the input to the next. So, when you invoke chain.invoke({'pet':'dog'}):\n",
        "\n",
        "The chat_template first formats the prompt with 'dog'.\n",
        "The formatted prompt is then sent to the chat (GPT-4) model, which generates a comma-separated list of dog names.\n",
        "Finally, list_output_parser takes that comma-separated string from the model and parses it into a clean Python list, making the output structured and easy to use programmatically.\n",
        "This creates a clear, modular, and reusable sequence for processing user input through an LLM to get a structured output."
      ],
      "metadata": {
        "id": "jy6uvCfriaF0"
      },
      "id": "jy6uvCfriaF0"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9cf969ef",
      "metadata": {
        "id": "9cf969ef"
      },
      "outputs": [],
      "source": [
        "chain = chat_template | chat | list_output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8718fc52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8718fc52",
        "outputId": "0359ec50-d4fd-43f3-b191-73e1993fce40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Max', 'Bella', 'Cooper']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "chain.invoke({'pet':'dog'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae598e1",
      "metadata": {
        "id": "4ae598e1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfcb559",
      "metadata": {
        "id": "bbfcb559"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain_env",
      "language": "python",
      "name": "langchain_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}